---
output: github_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)

Title <- "CBERS Data Cube - Compendium de replicação"
```

# CBERS Data Cube - Compendium de replicação

[![rc](https://img.shields.io/badge/research%20compendium-ready-brightgreen)](#)

Repositório com um `Executable Research Compendium` (ERC) para a geração de mapas de Uso e Cobertura da Terra utilizando Cubos de dados de imagens de Satélite.

Este ERC contém o código e o *workflow* necessário para a replicação da metodologia apresentada em:

> Picoli *et al*, (2020). `CBERS Data Cube: A powerful technology for mapping and monitoring brazilian biomes`. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XXIV ISPRS Congress (2020 edition) [10.5194/isprs-annals-V-3-2020-533-2020](https://doi.org/10.5194/isprs-annals-V-3-2020-533-2020)

## Conteúdo

O diretório **analysis** contém:

  - [:file\_folder: brick-creation](/analysis/brick-creation): Jupyter Notebok para a preparação dos dados para a classificação. Inicialmente o script realiza o *download* e recorte das cenas considerando uma região de interesse. Em seguida são calculados os índices espectrais EVI, GEMI, GNDVI, NDWI2 e PVR. Por fim, cada um dos *rasters* gerados são organizados no tempo, em arquivos únicos, formando **Raster Bricks** para a classificação;
  - [:file\_folder: brick-classification](/analysis/brick-classification): Jupyter-Notebooks para a classificação. Consumindo dos **Raster Bricks** criados com os scripts do `brick-creation`, os dados são classificador. Para isto, é feito a utilização do algoritmo Random Forest. O algoritmo é treinado considerando as séries temporais extraídas dos **Rasters Bricks**, associadas a um conjunto de amostras disponibilizada pelos autores do artigo que está sendo replicado;

O diretório **library** contém:

  - [:file\_folder: rep_cbers_cube](/library/rep_cbers_cube): Biblioteca de código escrita na linguagem Python, com todas as funções auxiliares utilizadas para a organização e processamento dos dados. Esta biblioteca é consumida pelos Jupyter Notebooks dos diretórios `brick-creation` e `brick-classification`.
  
O diretório **environment** contém:

  - [:file\_folder: R/Python Images](/environment): `Docker Images` conténdo os ambientes computacionais, com os requisitos necessários para a execução e reprodução desta replicação

O diretório **cwl** contém:

  - [:file\_folder: input/tools/workflows](/cwl): Para tornar mais fácil a execução, reprodução e possívelmente distribuição do fluxo de trabalho desenvolvido para a replicação, todas as etapas são escritas utilizando a **C**ommon **W**orkflow **L**anguage (CWL). Este diretório contém todos os *workflows*, *tools* e *inputs* definidos para a reprodução deste trabalho.

Por fim, no diretório **data** estão os  conjuntos de dados utilizados nos *scripts*. O diretório contém inicialmente apenas os dados de amostra. Todos os demais dados são gerados durante a execução dos scripts.

## Como fazer a execução

Esta replicação foi feita utilizando as linguagens de programação R e Python. Todas as dependências estão registradas dentro dos containers declarados no diretório [environment](/environment).

> Caso seja necessário, dentro de cada um dos ambientes, **R** e **Python**, estão disponíveis scripts `build.sh` que realizam a criação das imagens para a execução. Atualmente há disponível no DockerHub, ambas as imagens já prontas para uso, nos repositórios: [rep-cbers-dc-r](https://hub.docker.com/r/m3nin0/rep-cbers-dc-r) e [rep-cbers-dc-python](https://hub.docker.com/r/m3nin0/rep-cbers-dc-python).

Para fazer a execução, basta realizar utilizar uma engine de execução do CWL. O script `Makefile`, contém instruções para a utilização da engine de referência da linguagem CWL (`cwl-runner`). Para fazer sua utilização, basta utilizar os comandos abaixo:

*Instalação da cwltool*

```shell
pip install cwltool cwlref-runner
```

*Execução do workflow CWL*

```shell
make replicate
```

> Note que para a replicação, é necessário adicionar a variável de ambiente `BDC_ACCESS_TOKEN`. Após isto, a execução está pronta para ser realizada.

### Branches

O repositório `git` utilizado para versionar o ERC, contém duas *branches*:

- [master](https://github.com/M3nin0/experiment-rep-cbersdatacube): ERC com todo o material necessário para a replicação do artigo `CBERS Data Cube`;
- [rep-bdc-article](https://github.com/M3nin0/experiment-rep-cbersdatacube/tree/rep-bdc-article): ERC utilizado para exemplificar a possibilidade de reutilização de todo o material apresentado no ERC original. Para isso, os códigos e *workflows* criados são utilizados para a replicação de um dos mapas de uso e cobertura da Terra apresentados no artigo `Earth Observation Data Cubes for Brazil`

### Licenses

**Textos e figuras:**  [CC-BY-4.0](http://creativecommons.org/licenses/by/4.0/)

**Código :** See the [DESCRIPTION](DESCRIPTION) file

**Dados :** [CC-0](http://creativecommons.org/publicdomain/zero/1.0/) atribuição solicitada na reutilização

### Contribuições

Agradecemos as contribuições de todos. Antes de começar, consulte nossas [diretrizes para colaboradores](CONTRIBUTING.md). Este projeto é lançado com um [Código de Conduta do Contribuinte](CONDUCT.md). Ao participar deste projeto, você concorda em cumprir com seus termos.
